 program test_cufft
    use, intrinsic :: iso_c_binding 
    ! use filter
    use cufft_m
    use cudafor
    use mpi 
    use cpu_timer
    use dimen
    use precision
    use update_m
    use filter
    ! use filter_cpu
    implicit none

    real(fp),dimension(nx,ny,nz2) :: u_host1,dudx_host1,dudy_host1
    real(fp),dimension(nx,ny,nz2) :: v_host1,dvdx_host1,dvdy_host1
    real(fp),dimension(nx,ny,nz2) :: w_host1,dwdx_host1,dwdy_host1

    real(fp),dimension(nx,ny,nz2) :: u_host2,dudx_host2,dudy_host2
    real(fp),dimension(nx,ny,nz2) :: v_host2,dvdx_host2,dvdy_host2
    real(fp),dimension(nx,ny,nz2) :: w_host2,dwdx_host2,dwdy_host2

    real(fp),dimension(:,:,:),device,allocatable :: u,dudx,dudy
    real(fp),dimension(:,:,:),device,allocatable :: v,dvdx,dvdy
    real(fp),dimension(:,:,:),device,allocatable :: w,dwdx,dwdy
    
    integer,parameter :: nt = 100
    integer istat,i,j,k,t
    type(c_ptr) plan_batch(2)
    
    integer(kind=cuda_stream_kind) :: stream(3)
    type (cudaEvent) :: startEvent, stopEvent
    real :: time,time_cpu,max_error
    real(fp) :: wctime1,wctime2
    integer :: token,request
    integer :: localRank
    character(len=10) :: rankStr
    ! include 'dimen.h'
    ! MPI initialization
    
    call GET_ENVIRONMENT_VARIABLE ('OMPI_COMM_WORLD_LOCAL_RANK',rankStr)
    read (rankStr,'(i10)') localRank
    istat = cudaSetDevice (localRank)

    call mpi_init( ierr )
    call mpi_comm_rank( mpi_comm_world, me, ierr )
    call mpi_comm_size( mpi_comm_world, job_np, ierr )
    nall=mpi_comm_world

    call random_number(u_host1)
    call random_number(v_host1)
    call random_number(w_host1)

    allocate(u(nx,ny,nz2),dudx(nx,ny,nz2),dudy(nx,ny,nz2))
    allocate(v(nx,ny,nz2),dvdx(nx,ny,nz2),dvdy(nx,ny,nz2))
    allocate(w(nx,ny,nz2),dwdx(nx,ny,nz2),dwdy(nx,ny,nz2))

    u = u_host1
    v = v_host1
    w = w_host1

    !---------------------------------------------------------------------------
    ! GPU code
    !---------------------------------------------------------------------------


    ! Create cuda runtime event
    istat = cudaEventCreate(startEvent)
    istat = cudaEventCreate(stopEvent)

    ! Create fft plan
    call get_batchfft_plan(plan_batch,1)
    
    istat = cudaEventRecord(startEvent, 0)
    do t = 1,Nt

        call ddxy_filter(u,dudx,dudy,plan_batch)

        call update_uv_async(u,request,me,nall)
        call update_uv_async(dudx,request,me,nall)
        call update_uv_async(dudy,request,me,nall)

        call ddxy_filter(v,dvdx,dvdy,plan_batch)
        
        call update_uv_async(v,request,me,nall)
        call update_uv_async(dvdx,request,me,nall)
        call update_uv_async(dvdy,request,me,nall)

        call ddxy_filter(w,dwdx,dwdy,plan_batch)
        
        call update_uv_async(w,request,me,nall)
        call update_uv_async(dwdx,request,me,nall)
        call update_uv_async(dwdy,request,me,nall)

        ! call mpi_wait(request, STATUS2, ierr)
        call mpi_barrier(nall,ierr)

    end do   
    istat = cudaEventRecord(stopEvent, 0)
    istat = cudaEventSynchronize(stopEvent)
    istat = cudaEventElapsedTime(time, startEvent, stopEvent)

    call cufftDestroy(plan_batch(1))
    call cufftDestroy(plan_batch(2))   

    u_host2 = u 
    print *,u_host2(10,10,10)
    ! dudx_host2 = dudx

    WRITE (*,*) 'GPU time: ', time/nt, ' ms'
    !---------------------------------------------------------------------------
    ! cpu code
    !---------------------------------------------------------------------------
    ! call ddxy_filter_cpu(u_host1,dudx_host1,dudy_host1,0)
    ! call get_walltime(wctime1)
    ! do t = 1,Nt
    !     call ddxy_filter_cpu(u_host1,dudx_host1,dudy_host1,1)
    !     call ddxy_filter_cpu(v_host1,dvdx_host1,dvdy_host1,1)
    !     call ddxy_filter_cpu(w_host1,dwdx_host1,dwdy_host1,1)
    ! end do
    ! call get_walltime(wctime2)

    ! max_error = maxval(abs(dudx_host1(:,:,2:nzb+1)-dudx_host2(:,:,2:nzb+1)))
    ! print *,'Max error',max_error
    
    ! time_cpu = (wctime2 - wctime1)
    ! WRITE (*,*) 'GPU/CPU Speedup: ', time_cpu/time*1000 
    call MPI_finalize(ierr)
end program test_cufft

